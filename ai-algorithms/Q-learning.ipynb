{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 6\n",
        "\n",
        "Celem ćwiczenia jest implementacja algorytmu Q-learning.\n",
        "\n",
        "Następnie należy stworzyć agenta rozwiązującego problem [Taxi](https://gymnasium.farama.org/environments/toy_text/taxi/). Problem dostępny jest w pakiecie **gym**.\n",
        "\n",
        "Punktacja (max 8 pkt):\n",
        "- Implementacja algorytmu Q-learning. [3 pkt]\n",
        "- Eksperymenty dla różnych wartości hiperparametrów [2 pkt]\n",
        "- Jakość kodu [1.5 pkt]\n",
        "- Wnioski [1.5 pkt]\n"
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import pygame\n",
        "from IPython.display import clear_output\n",
        "\n",
        "random.seed(1234)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interfejs\n",
        "\n",
        "class QLearningSolver:\n",
        "    \"\"\"Class containing the Q-learning algorithm that might be used for different discrete environments.\"\"\"\n",
        "    def __init__(self,\n",
        "                 observation_space:int,\n",
        "                 action_space:int,\n",
        "                 learning_rate:float=0.1,\n",
        "                 gamma:float=0.9,\n",
        "                 epsilon:float=0.1,\n",
        "                 ):\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma # wsp znizki nagrody\n",
        "        self.epsilon = epsilon # eksploracja\n",
        "        self.q_table = np.zeros((observation_space, action_space))\n",
        "        self.steps = []\n",
        "\n",
        "    def __call__(self, state:np.ndarray, action:np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Return Q-value of given state and action.\"\"\"\n",
        "        return self.q_table[state, action]\n",
        "\n",
        "    def update(self, state:np.ndarray, action:np.ndarray, next_state:np.ndarray, reward:float) -> None:\n",
        "        # state jako aktualny stan agenta, akcja jako czynnosc wykonywana przez agenta\n",
        "        \"\"\"Update Q-value of given state and action.\"\"\"\n",
        "        val = reward + self.gamma * np.max(self.q_table[next_state])\n",
        "        act = (1-self.learning_rate) * self.q_table[state, action]\n",
        "        self.q_table[state, action] = act + self.learning_rate * val\n",
        "\n",
        "    def get_best_action(self, state:np.ndarray):\n",
        "        \"\"\"Return action that maximizes Q-value for a given state.\"\"\"\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Elegant representation of Q-learning solver.\"\"\"\n",
        "        return(\"A\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def learn(self, episodes:int, iter_per_episode:int, env):\n",
        "        for episode in(range(episodes)):\n",
        "            state = env.reset()[0]\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            while not done:\n",
        "                if(np.random.uniform(0, 1) < self.epsilon):\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    action = self.get_best_action(state)\n",
        "                \n",
        "                next_state, reward, done, info, a = env.step(action)\n",
        "\n",
        "                self.update(state, action, next_state, reward)\n",
        "                episode_reward += 1\n",
        "                \n",
        "                state = next_state\n",
        "\n",
        "            self.steps.append(episode_reward)\n",
        "\n",
        "            self.epsilon = max(0.01, np.exp(-0.001*episode))"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find best learning rate\n",
        "\n",
        "\n",
        "learning_rate = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "gamma:float=0.4 #discount rate\n",
        "epsilon:float=0.1 #exploration rate\n",
        "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
        "\n",
        "for nn in range(9):\n",
        "    qlearn = QLearningSolver(streets.observation_space.n, streets.action_space.n, learning_rate[nn], gamma, epsilon)\n",
        "    qlearn.learn(10000, 25, streets)\n",
        "    print(np.mean(qlearn.steps[9000:10000]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "16.038\n13.382\n13.289\n13.155\n13.229\n13.331\n13.274\n13.173\n13.133\n"
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Find best discount rate\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "gamma:float=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] #discount rate\n",
        "epsilon:float=0.1 #exploration rate\n",
        "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
        "\n",
        "for nn in range(9):\n",
        "    qlearn = QLearningSolver(streets.observation_space.n, streets.action_space.n, learning_rate, gamma[nn], epsilon)\n",
        "    qlearn.learn(10000, 25, streets)\n",
        "    print(np.mean(qlearn.steps[9000:10000]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "23.655\n21.079\n18.112\n16.053\n14.898\n13.998\n13.548\n13.292\n13.124\n"
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Find best exp rate\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "gamma:float = 0.1 #discount rate\n",
        "epsilon:float=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] #exploration rate\n",
        "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
        "\n",
        "for nn in range(9):\n",
        "    qlearn = QLearningSolver(streets.observation_space.n, streets.action_space.n, learning_rate, gamma, epsilon[nn])\n",
        "    qlearn.learn(10000, 25, streets)\n",
        "    print(np.mean(qlearn.steps[9000:10000]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "23.485\n23.477\n23.757\n23.926\n23.572\n23.601\n23.559\n23.66\n23.656\n"
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate:float=0.5\n",
        "gamma:float=0.9 #discount rate\n",
        "epsilon:float=0.1 #exploration rate\n",
        "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
        "\n",
        "qlearn = QLearningSolver(streets.observation_space.n, streets.action_space.n, learning_rate, gamma, epsilon)\n",
        "qlearn.learn(10000, 25, streets)\n",
        "for i in range(10):\n",
        "    print(np.mean(qlearn.steps[1000*i:1000*(i+1)]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "c:\\Users\\niewi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "168.019\n17.842\n14.512\n13.576\n13.315\n13.207\n13.245\n13.178\n13.114\n13.104\n"
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eksperymenty"
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wnioski"
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "1. Dzięki przeprowadzaniu uczenia się, algorytm jest w stanie znaleźć rozwiązanie problemu taxi, widać to po uśrednionej ilości kroków do osiągięcia celu,\n",
        "w początkowej fazie liczba ta oscyluje w granicach 25 co oznacza że algorytm nie znajduje rozwiązania zadania, następnie sukcesywnie się zmniejsza i osiąga wartość około 13.\n",
        "2. Optymalna ilość epok wynosi około 10.000, zwiększenie tej wartości do 30.000 nie poprawia działania alg., wartość średnia ilości kroków nie zmniejsza się w sposób znaczny\n",
        "3. Dobór poszczególnych współczynników jest kluczowy dla poprawnego działania algorytmu, z przeprowadzonych ekperymentów wynika iż optymalne wartości to:\n",
        "learning rate = 0.3\n",
        "gamma = 0.9\n",
        "epsilon = 0.7"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d22d1cca6b4746dfc6f33d99aa9b3bf3881c3b5662deffe893203eefc35bb98d"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}